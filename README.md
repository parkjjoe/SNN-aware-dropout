# SNN-dropout-custom
## Spiking Neural Networks
Spiking Neural Networks (SNNs) is a 3rd generation neuromorphic artificial intelligence technology and a new paradigm in the field of neural networks. It has evolved from the first-generation Perceptron and second-generation Artificial Neural Networks. Unlike traditional deep learning models (DNNs), it attempts to more precisely simulate the biological characteristics of neurons and processes input information as temporal signals. Neurons in SNNs transmit information through discrete signals called ‘spikes’ instead of continuous values. Because information is transmitted through spikes only when neurons are activated, it is more energy efficient than traditional DNNs.
## Dropout
Among the layers that make up the neural network model structure, Dropout is a layer that prevents the network from being overly dependent on specific neurons by randomly deactivating some neurons during the learning process. The most common reason why dropout layers are used is to improve overfitting. Overfitting refers to a phenomenon in which a model relies too much on training data, resulting in poor generalization performance for new data. By adding a dropout layer, the model can learn various characteristics of the data and ultimately improve the model's generalization performance. In particular, in DNNs, the great effect of dropout can be expected, and the robustness of the network can be improved by randomly removing some neurons across multiple layers.
## Dropout in SNNs
However, in the case of SNNs, it is difficult to apply this traditional approach of dropout. Unlike general DNNs, SNNs attempt to physically replicate human brain signals and deliver information based on the activity patterns of neurons over time. Therefore, a spike with a value of 1 is generated only when the neuron's membrane potential exceeds a certain threshold. In SNNs, the output value after the activation layer using an activation function that imparts non-linearity to the output of the previous layer and passes it to the next layer appears as 0 in most cases. Therefore, when general dropout technology is applied to SNNs, the effect is minimal for neurons that are not already activated and do not generate spikes, and for neurons that are activated and generate spikes, the information contained through the generated spikes is lost. Additional removal may have a negative impact on learning. Therefore, a different approach from the existing dropout method was needed in SNNs, and research on controlling the removal probability using the membrane potential of neurons and research on controlling the removal probability using synapse weights, etc. were carried out.   
So I propose new dropout technologies that can replace the existing dropout and is inspired by its function, adjusting the removal probability according to the firing rate of the neuron.
